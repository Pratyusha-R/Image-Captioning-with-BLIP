# Install the transformers library
# !pip install transformers

'''
This line, when uncommented, installs the transformers library
from Hugging Face, which contains models like BLIP (used here for 
image captioning).
'''


# Importing libraries
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import torch
import gradio as gr
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image

'''
1. BlipProcessor and BlipForConditionalGeneration are specific components 
for BLIP (Bootstrapping Language-Image Pre-training) models, which can 
process and generate captions for images.
2. PIL is a library used to open, manipulate, and process images.
3. torch (PyTorch) helps perform tensor computations, which this model requires.
4. gradio allows creating a web-based interface for users to interact with your code visually.
'''

# Loading the model and processor
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

'''
These lines load the BLIP model and processor from the pretrained model repository 
by Salesforce. The processor formats image data so itâ€™s compatible with the model, 
while the model itself is a conditional generation model that creates text captions 
based on images.
'''

# Defining the generate_caption function
def generate_caption(image):
    # Now directly using the PIL Image object
    inputs = processor(images=image, return_tensors="pt")
    outputs = model.generate(**inputs)
    caption = processor.decode(outputs[0], skip_special_tokens=True)
    return caption

'''
Image Input: This function takes an image (in PIL format) and processes it for the model.
Processing the Image: processor converts the image into a format the model can understand (inputs), 
returning a PyTorch tensor (indicated by return_tensors="pt").
Generating Output: The model then generates a caption based on the input image, returning a 
sequence of encoded tokens (outputs).
Decoding the Caption: Finally, processor.decode converts the output tokens back to text, 
producing the final caption.
'''

# Defining caption_image for robustness
def caption_image(image):
    """
    Takes a PIL Image input and returns a caption.
    """
    try:
        caption = generate_caption(image)
        return caption
    except Exception as e:
        return f"An error occurred: {str(e)}"

'''
This function wraps generate_caption in a try-except block, which makes the code more resilient. 
If an error occurs, it returns a message with the error details, instead of crashing.
'''

# Creating Gradio Interface
iface = gr.Interface(
    fn=caption_image,
    inputs=gr.Image(type="pil"),
    outputs="text",
    title="Image Captioning with BLIP",
    description="Upload an image to generate a caption."
)

'''
Function (fn): Specifies that caption_image is the main function to be run when a user interacts with this interface.
Inputs: Defines that the input is an image, in PIL format.
Outputs: Sets the output to be text (the generated caption).
Title and Description: Provides a title and description to guide users.
'''

# Launching the interface
iface.launch()

'''
This command launches the Gradio interface, opening a web page where users can upload images and get captions generated by your model.
'''